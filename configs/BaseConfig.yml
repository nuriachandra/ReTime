# Configuration file for training BaseTimeTransformer model

# Data configuration
data_path: "data/synthetic_data/constant_series1.npy"  # Path to the time series data
normalize_data: false               # Whether to normalize the data
train_ratio: 0.7                   # Ratio of data for training
val_ratio: 0.15                    # Ratio of data for validation
# test_ratio is 1 - train_ratio - val_ratio

# Model configuration
model: 'BaseTimeTransformer'
block_size: 104                    # Sequence length (input window size)
n_layer: 3                         # Number of transformer layers. Currently not used by recurrent architectures
n_head: 4                          # Number of attention heads
n_embd: 512                        # Embedding dimension
h: 2                               # Horizon length (prediction steps)
max_recurrence: 10                 # Max number of recurrences when the number of recurrences is selected randomly. Only used by recurrent architectures.
injection_type: ~                  # Method for embedding injection
dropout: 0                         # Dropout rate
bias: true                         # Whether to use bias in Linear layers


# Training configuration
output_dir: "experiments/synthetic/"               # Directory to save outputs
seed: 42                           # Random seed for reproducibility
use_gpu: true                      # Whether to use GPU for training
batch_size: 20                     # Batch size
epochs: 10                         # Maximum number of epochs
learning_rate: 0.0001              # Learning rate
weight_decay: 0.0001               # Weight decay for regularization
gradient_clip: 1.0                 # Maximum gradient norm
patience: 10                       # Early stopping patience

hydra:
  run:
    dir: .
