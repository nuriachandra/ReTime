# Configuration file for training BaseTimeTransformer model

# Data configuration
data_path: "data/synthetic_data/test.npy"  # Path to the time series data
normalize_data: false               # Whether to normalize the data
train_ratio: 0.7                   # Ratio of data for training
val_ratio: 0.15                    # Ratio of data for validation
# test_ratio is 1 - train_ratio - val_ratio

# Model configuration
model: 'BaseTimeTransformer'
block_size: 126                    # Sequence length (input window size)
n_layer: 3                         # Number of transformer layers. Currently not used by recurrent architectures
n_head: 4                          # Number of attention heads
n_embd: 512                        # Embedding dimension
h: 2                               # Horizon length (prediction steps)
max_recurrence: 10                 # Max number of recurrences when the number of recurrences is selected randomly. Only used by recurrent architectures.
injection_type: ~                  # Method for embedding injection
dropout: 0                         # Dropout rate
bias: true                         # Whether to use bias in Linear layers
out_style: 'linear_proj'           # options: 'linear_proj', 'ext' for extension of the time dimension and take the last h

# Training configuration
output_dir: "experiments/synthetic/"               # Directory to save outputs
set_seed: true                     # Random seed for reproducibility
seed: 42
use_gpu: true                      # Whether to use GPU for training
batch_size: 20                     # Batch size
epochs: 10                         # Maximum number of epochs
learning_rate: 0.0001              # Learning rate
weight_decay: 0.0001               # Weight decay for regularization
gradient_clip: 1.0                 # Maximum gradient norm
patience: 10                       # Early stopping patience

wandb:
  use: false
  proj: "ReTime"

hydra:
  run:
    dir: .
